{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d57a32",
   "metadata": {},
   "source": [
    "#  Fitting a Convolutional Autoencoder (CNN-AE) with Pytorch\n",
    "\n",
    "\n",
    "## Import libraries, including PyTorch\n",
    "Setup Pytorch guidance is here: https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "import torch\n",
    "torch.set_num_threads(2)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F                # contains activation functions, sampling layers etc\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid    \n",
    "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
    "import torch.optim as optim                    # For optimization routines such as SGD, ADAM, ADAGRAD, etc\n",
    "from torch.utils.data import DataLoader,random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c2436",
   "metadata": {},
   "source": [
    "### Load in the MNIST digits data\n",
    "\n",
    "Like yesterday, we use the MNIST digits dataset. We split the training dataset into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset.transform = train_transform\n",
    "test_dataset.transform = test_transform\n",
    "\n",
    "m = len(train_dataset)\n",
    "\n",
    "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09dece",
   "metadata": {},
   "source": [
    "## View basic data set information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac8eb9",
   "metadata": {},
   "source": [
    "### Below we create the encode and decoder objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, \n",
    "            stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
    "            padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632ba25",
   "metadata": {},
   "source": [
    "### Below we define the loss function, learning rate and latent space dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03170906",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr= 0.001\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks and set the latent dimension \n",
    "d = 4\n",
    "\n",
    "#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfb78e",
   "metadata": {},
   "source": [
    "### Below we define how the training is done and collect the training loss from each batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35daceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Move tensor to the proper device\n",
    "        image_batch = image_batch.to(device)\n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_batch)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22955bbe",
   "metadata": {},
   "source": [
    "### We also compute the loss over validation data to determine when is a good time to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = image_batch.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4cd54",
   "metadata": {},
   "source": [
    "### Below is a function to plot the images recontructed by the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_outputs(encoder,decoder,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    targets = test_dataset.targets.numpy()\n",
    "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = test_dataset[t_idx[i]][0].unsqueeze(0).to(device)\n",
    "      encoder.eval()\n",
    "      decoder.eval()\n",
    "      with torch.no_grad():\n",
    "         rec_img  = decoder(encoder(img))\n",
    "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae7a63",
   "metadata": {},
   "source": [
    "###Â Below is some code to train for a number of epochs (an epoch is one complete pass through the data) \n",
    "\n",
    "To save time we have commented this out and loaded in the trained parameters in the next block. The loaded in data was obtained by training for 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 20\n",
    "# diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "# for epoch in range(num_epochs):\n",
    "#    train_loss = train_epoch(encoder,decoder,device,\n",
    "#    train_loader,loss_fn,optim)\n",
    "#    val_loss = test_epoch(encoder,decoder,device,test_loader,loss_fn)\n",
    "#    print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "#    diz_loss['train_loss'].append(train_loss)\n",
    "#    diz_loss['val_loss'].append(val_loss)\n",
    "#    plot_ae_outputs(encoder,decoder,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code below to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9702b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), './saved_models/cnn_ae_encoder')\n",
    "# torch.save(decoder.state_dict(), './saved_models/cnn_ae_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32138ddd",
   "metadata": {},
   "source": [
    "### Below we load in the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef12e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "encoder.load_state_dict(torch.load('./saved_models/cnn_ae_encoder'))\n",
    "decoder.load_state_dict(torch.load('./saved_models/cnn_ae_decoder'))\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "plot_ae_outputs(encoder,decoder,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d864e",
   "metadata": {},
   "source": [
    "### Below is the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f39484",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch(encoder,decoder,device,test_loader,loss_fn).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbe81a",
   "metadata": {},
   "source": [
    "### Below we plot the training and validation loss\n",
    "\n",
    "This is commented out since we have pre-trained the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "# plt.semilogy(diz_loss['train_loss'], label='Train')\n",
    "# plt.semilogy(diz_loss['val_loss'], label='Valid')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Average Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad793671",
   "metadata": {},
   "source": [
    "### Below we draw some images from random points in the latent space. \n",
    "\n",
    "Since the autoencoder latent space can be quite irregular and therefore some of the generated samples don't look so good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # calculate mean and std of latent code, generated takining in test images as inputs \n",
    "    testiter = iter(test_loader)\n",
    "    images, labels = next(testiter)\n",
    "    images = images.to(device)\n",
    "    latent = encoder(images)\n",
    "    latent = latent.cpu()\n",
    "\n",
    "    mean = latent.mean(dim=0)\n",
    "    print(f\"means: {mean}\")\n",
    "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
    "    print(f\"standard deviations: {std}\")\n",
    "\n",
    "    # sample latent vectors from the normal distribution\n",
    "    latent = torch.randn(128, d)*std + mean\n",
    "\n",
    "    # reconstruct images from the random latent vectors\n",
    "    latent = latent.to(device)\n",
    "    img_recon = decoder(latent)\n",
    "    img_recon = img_recon.cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
    "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63308e46",
   "metadata": {},
   "source": [
    "### Below we look at where the different digits lie in the latent space\n",
    "\n",
    "Note that the model is unsupervised and the labels were not used for training, but it can be observed that there is some reasonable clustering of the digits in the latent space. \n",
    "\n",
    "We have now put the encoded test data in a pandas dataframe to use the methods from the unsupervised learning labs to cluster and visualise the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_size = 1000 # We just take 1000 samples from the test data to keep computation time down\n",
    "encoded_samples = []\n",
    "for i in tqdm(range(1,test_subset_size+1)):\n",
    "    sample_idx = torch.randint(len(test_dataset), size=(1,)).item()\n",
    "    sample = test_dataset[sample_idx]\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    \n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea0b3b",
   "metadata": {},
   "source": [
    "### Below we put the labels in the index of the dataframe\n",
    "\n",
    "Now the encoded test data is ready to be clustered or visualised as you wish. The format of the data is the same as you used in the PCA and clustering labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6640f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encoded_samples.set_index('label')\n",
    "labels = encoded_data.index.astype(str)\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b3fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
